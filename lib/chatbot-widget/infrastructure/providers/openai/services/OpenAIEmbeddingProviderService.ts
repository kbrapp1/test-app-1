/**
 * AI Instructions: OpenAI Embedding Provider Service for text vectorization
 * - Handle single and batch embedding API calls with comprehensive logging
 * - Manage API timeouts, error handling, and response processing
 * - Keep under 250 lines by focusing on API operations only
 * - Follow @golden-rule patterns with single responsibility principle
 */

import OpenAI from 'openai';
import {
  EmbeddingResult,
  EmbeddingLogContext,
  OpenAIEmbeddingRequest,
  OpenAIEmbeddingResponse,
  EMBEDDING_CONSTANTS
} from '../../../../domain/services/interfaces/EmbeddingTypes';

export class OpenAIEmbeddingProviderService {
  private client: OpenAI;
  private logContext?: EmbeddingLogContext;

  constructor(apiKey: string, logContext?: EmbeddingLogContext) {
    this.client = new OpenAI({
      apiKey,
      timeout: EMBEDDING_CONSTANTS.API_TIMEOUT
    });
    this.logContext = logContext;
  }

  /** Set logging context for API call logging */
  setLogContext(logContext: EmbeddingLogContext): void {
    this.logContext = logContext;
  }

  /** Generate single embedding via OpenAI API */
  async generateSingleEmbedding(text: string): Promise<EmbeddingResult> {
    this.log('üîÑ Single embedding API call initiated');
    this.logApiCallStart('SINGLE');

    try {
      const apiRequest: OpenAIEmbeddingRequest = {
        model: EMBEDDING_CONSTANTS.DEFAULT_MODEL,
        input: text.trim(),
        encoding_format: 'float'
      };

      this.logApiRequest(apiRequest, text);

      const startTime = Date.now();
      const response = await this.client.embeddings.create(apiRequest);
      const duration = Date.now() - startTime;

      this.logApiResponse(response, duration);

      const result: EmbeddingResult = {
        embedding: response.data[0].embedding,
        text: text.trim(),
        tokenCount: response.usage.total_tokens
      };

      this.logEmbeddingResult(result);
      this.logApiCallEnd();

      return result;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      this.log(`‚ùå Single embedding API call failed: ${errorMessage}`);
      this.logApiCallEnd();
      throw new Error(`Failed to generate embedding: ${errorMessage}`);
    }
  }

  /** Generate batch embeddings via OpenAI API */
  async generateBatchEmbeddings(texts: string[]): Promise<EmbeddingResult[]> {
    const filteredTexts = texts.filter(t => t.trim().length > 0);
    
    if (filteredTexts.length === 0) {
      return [];
    }

    this.log(`üîÑ Batch embedding API call initiated for ${filteredTexts.length} texts`);
    this.logApiCallStart('BATCH');

    try {
      const apiRequest: OpenAIEmbeddingRequest = {
        model: EMBEDDING_CONSTANTS.DEFAULT_MODEL,
        input: filteredTexts,
        encoding_format: 'float'
      };

      this.logBatchApiRequest(apiRequest, filteredTexts);

      const startTime = Date.now();
      const response = await this.client.embeddings.create(apiRequest);
      const duration = Date.now() - startTime;

      this.logApiResponse(response, duration);

      const results = response.data.map((item, index) => ({
        embedding: item.embedding,
        text: filteredTexts[index].trim(),
        tokenCount: Math.round(response.usage.total_tokens / filteredTexts.length) // Approximate
      }));

      this.logBatchEmbeddingResults(results, response.usage.total_tokens);
      this.logApiCallEnd();

      return results;
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      this.log(`‚ùå Batch embedding API call failed: ${errorMessage}`);
      this.logApiCallEnd();
      throw new Error(`Failed to generate batch embeddings: ${errorMessage}`);
    }
  }

  /** Log API call start with visual separation */
  private logApiCallStart(callType: 'SINGLE' | 'BATCH'): void {
    this.log('üîó =====================================');
    this.log(`üîó OPENAI EMBEDDINGS API CALL - ${callType}`);
    this.log('üîó =====================================');
  }

  /** Log API call end with visual separation */
  private logApiCallEnd(): void {
    this.log('üîó =====================================');
  }

  /** Log API request details with redacted sensitive information */
  private logApiRequest(request: OpenAIEmbeddingRequest, text: string): void {
    this.log('üì§ COMPLETE API REQUEST:');
    this.log('üîó Endpoint: https://api.openai.com/v1/embeddings');
    this.log('üìã Request Headers:');
    this.log(JSON.stringify({
      'Content-Type': 'application/json',
      'Authorization': 'Bearer [REDACTED]',
      'User-Agent': 'Chatbot-Widget-Embeddings/1.0'
    }, null, 2));
    this.log('üìã Request Body:');
    this.log(JSON.stringify(request, null, 2));
    this.log(`üìã Input Text Length: ${text.length} characters`);
    this.log(`üìã Input Text Preview: "${text.substring(0, 100)}${text.length > 100 ? '...' : ''}"`);
  }

  /** Log batch API request details with text previews */
  private logBatchApiRequest(request: OpenAIEmbeddingRequest, texts: string[]): void {
    this.log('üì§ COMPLETE API REQUEST:');
    this.log('üîó Endpoint: https://api.openai.com/v1/embeddings');
    this.log('üìã Request Headers:');
    this.log(JSON.stringify({
      'Content-Type': 'application/json',
      'Authorization': 'Bearer [REDACTED]',
      'User-Agent': 'Chatbot-Widget-Embeddings/1.0'
    }, null, 2));
    this.log('üìã Complete Request Body:');
    this.log(JSON.stringify(request, null, 2));
    this.log('üìã Batch Items Being Vectorized:');
    texts.forEach((text, index) => {
      this.log(`üìÑ Item ${index + 1}:`);
      this.log(`   Text: "${text}"`);
      this.log(`   Length: ${text.length} characters`);
      this.log('');
    });
  }

  /** Log API response details with timing and usage metadata */
  private logApiResponse(response: OpenAIEmbeddingResponse, duration: number): void {
    this.log(`‚úÖ API Call Completed: ${new Date().toISOString()}`);
    this.log(`‚è±Ô∏è  Duration: ${duration}ms`);
    this.log('üì• COMPLETE API RESPONSE:');
    this.log('üìã Response Headers:');
    this.log(JSON.stringify({
      'content-type': 'application/json',
      'openai-model': response.model || 'N/A',
      'openai-version': 'N/A'
    }, null, 2));
    this.log('üìã Response Summary:');
    this.log(JSON.stringify({
      model: response.model,
      usage: response.usage,
      data_count: response.data.length,
      embedding_dimensions: response.data[0]?.embedding?.length || 0
    }, null, 2));
  }

  /** Log single embedding result with vector dimensions and token usage */
  private logEmbeddingResult(result: EmbeddingResult): void {
    this.log('üîß EMBEDDING RESULT:');
    this.log(`üìä Vector Dimensions: ${result.embedding.length}`);
    this.log(`üìä Token Count: ${result.tokenCount}`);
    this.log(`üìä Text Length: ${result.text.length} characters`);
  }

  /** Log batch embedding results with aggregate statistics */
  private logBatchEmbeddingResults(results: EmbeddingResult[], totalTokens: number): void {
    this.log('üîß BATCH EMBEDDING RESULTS:');
    this.log(`üìä Embeddings Generated: ${results.length}`);
    if (results.length > 0) {
      this.log(`üìä Average Vector Dimensions: ${results[0].embedding.length}`);
    }
    this.log(`üìä Total Tokens Used: ${totalTokens}`);
    this.log(`üìä Average Tokens per Text: ${Math.round(totalTokens / results.length)}`);
  }

  /** Log entry with fallback to no-op if no context */
  private log(message: string): void {
    if (this.logContext) {
      this.logContext.logEntry(message);
    }
  }
} 